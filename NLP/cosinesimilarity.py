# -*- coding: utf-8 -*-
"""Group5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XU6IRQnYsTvDH7Owk8reusrS7MuN2SkG
"""

import nltk
nltk.download("all")

import gensim

from google.colab import drive
drive.mount('/content/drive')

with open('/content/drive/My Drive/1.txt', 'r') as f: 
    print(f.read())

with open('/content/drive/My Drive/1.txt', 'r') as f: 
    print(f.read())
    print("==========")
with open('/content/drive/My Drive/2.txt', 'r') as g: 
    print(g.read())
    print("==========")
with open('/content/drive/My Drive/3.txt', 'r') as h: 
    print(h.read())
    print("==========")
with open('/content/drive/My Drive/4.txt', 'r') as i: 
    print(i.read())
    print("==========")

a = open('1.txt',"r").readlines()
print(a)
print("===")
a1 = " ".join([x.strip() for x in a])
print(a1)

b = open('2.txt',"r").readlines()
print(b)
b1 = " ".join([x.strip() for x in b])
print(b1)

c = open('3.txt',"r").readlines()
print(c)
c1 = " ".join([x.strip() for x in c])
print(c1)

d = open('4.txt',"r").readlines()
print(d)
d1 = " ".join([x.strip() for x in d])
print(d1)

"""# *Generate Tokens*

### 1.txt
"""

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

file_docs1 = []

with open ('1.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file_docs1.append(line)

print("Number of documents:",len(file_docs1))

print(file_docs1)
gen_docs1 = [[w.lower() for w in word_tokenize(text)] 
            for text in file_docs1]
print(gen_docs1)

"""## 2.txt"""

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

file_docs2 = []

with open ('2.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file_docs2.append(line)

print("Number of documents:",len(file_docs2))

print(file_docs2)
gen_docs2 = [[w.lower() for w in word_tokenize(text)] 
            for text in file_docs2]
print(gen_docs2)

"""## 3.txt"""

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

file_docs3 = []

with open ('3.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file_docs3.append(line)

print("Number of documents:",len(file_docs3))

print(file_docs3)
gen_docs3 = [[w.lower() for w in word_tokenize(text)] 
            for text in file_docs3]
print(gen_docs3)

"""### 4.txt"""

file_docs4 = []

with open ('4.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file_docs4.append(line)

print("Number of documents:",len(file_docs4))

print(file_docs4)
gen_docs4 = [[w.lower() for w in word_tokenize(text)] 
            for text in file_docs4]
print(gen_docs4)

"""# ==============================================================================================================================================

## Making a dictionary of tokens
"""

# 1.txt
dictionary1 = gensim.corpora.Dictionary(gen_docs1)
print(dictionary1.token2id)

# 2.txt
dictionary2 = gensim.corpora.Dictionary(gen_docs2)
print(dictionary2.token2id)

# 3.txt
dictionary3 = gensim.corpora.Dictionary(gen_docs3)
print(dictionary3.token2id)

# 4.txt
dictionary4 = gensim.corpora.Dictionary(gen_docs4)
print(dictionary4.token2id)

"""# Making a bag of words
### The next important object you need to familiarize with in order to work in gensim is the Corpus (a Bag of Words). It is a basically object that contains the word id and its frequency in each document (just lists the number of times each word occurs in the sentence).

### Note that, a ‘token’ typically means a ‘word’. A ‘document’ can typically refer to a ‘sentence’ or ‘paragraph’ and a ‘corpus’ is typically a ‘collection of documents as a bag of words’.

### Now, create a bag of words corpus and pass the tokenized list of words to the Dictionary.doc2bow()

### (a,b) a=>id, b=>count in the document
"""

# 1.txt
corpus1 = [dictionary1.doc2bow(gen_doc) for gen_doc in gen_docs1]
print(corpus1)

# 2.txt
corpus2 = [dictionary2.doc2bow(gen_doc) for gen_doc in gen_docs2]
print(corpus2)

# 3.txt
corpus3 = [dictionary3.doc2bow(gen_doc) for gen_doc in gen_docs3]
print(corpus3)

# 4.txt
corpus4 = [dictionary4.doc2bow(gen_doc) for gen_doc in gen_docs4]
print(corpus4)

"""# TFIDF
##### Term Frequency – Inverse Document Frequency(TF-IDF) is also a bag-of-words model but unlike the regular corpus, TFIDF down weights tokens (words) that appears frequently across documents.

##### Tf-Idf is calculated by multiplying a local component (TF) with a global component (IDF) and optionally normalizing the result to unit length. Term frequency is how often the word shows up in the document and inverse document frequency scales the value by how rare the word is in the corpus. In simple terms, words that occur more frequently across the documents get smaller weights.
"""

# 1.txt
import numpy as np
tfidf1 = gensim.models.TfidfModel(corpus1)
print(corpus1)
print("")
for doc in tfidf1[corpus1]:
    print([[dictionary1[id], np.around(freq, decimals=2)] for id, freq in doc])

# 2.txt
import numpy as np
tfidf2 = gensim.models.TfidfModel(corpus2)
print(corpus2)
print("")
for doc in tfidf2[corpus2]:
    print([[dictionary2[id], np.around(freq, decimals=2)] for id, freq in doc])

# 3.txt
import numpy as np
tfidf3 = gensim.models.TfidfModel(corpus3)
print(corpus3)
print("")
for doc in tfidf3[corpus3]:
    print([[dictionary3[id], np.around(freq, decimals=2)] for id, freq in doc])

# 4.txt
import numpy as np
tfidf4 = gensim.models.TfidfModel(corpus4)
print(corpus4)
print("")
for doc in tfidf4[corpus4]:
    print([[dictionary4[id], np.around(freq, decimals=2)] for id, freq in doc])

"""#Creating similarity measure object
##### Now, we are going to create similarity object. The main class is Similarity, which builds an index for a given set of documents.The Similarity class splits the index into several smaller sub-indexes, which are disk-based. Let's just create similarity object then you will understand how we can use it for comparing.
"""

# building the index
sims1 = gensim.similarities.Similarity('workdir1/',tfidf1[corpus1],
                                        num_features=len(dictionary1))

# building the index
sims1 = gensim.similarities.Similarity('/content/drive',tfidf1[corpus1],
                                        num_features=len(dictionary1))

file2_docs = []

with open ('2.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file2_docs.append(line)
# with open ('3.txt') as f:
#     tokens = sent_tokenize(f.read())
#     for line in tokens:
#         file2_docs.append(line)
# with open ('3.txt') as f:
#     tokens = sent_tokenize(f.read())
#     for line in tokens:
#         file2_docs.append(line)

print("Number of documents:",len(file2_docs))  
for line in file2_docs:
    query_doc = [w.lower() for w in word_tokenize(line)]
    query_doc_bow = dictionary1.doc2bow(query_doc) 
    #update an existing dictionary and create bag of words

"""# Document similarities to query
At this stage, you will see similarities between the query and all index documents. To obtain similarities of our query document against the indexed documents:
"""

# perform a similarity query against the corpus
query_doc_tf_idf = tfidf1[query_doc_bow]
# print(document_number, document_similarity)
print('Comparing Result:', sims1[query_doc_tf_idf])

"""# Cosine measure returns similarities in the range (the greater, the more similar).

#Calculating average similarity
"""

import numpy as np

sum_of_sims =(np.sum(sims1[query_doc_tf_idf], dtype=np.float32))
print(sum_of_sims)

percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))
print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')
print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')
print(f'Average similarity rounded percentage: {percentage_of_similarity}')

"""# Here we conclude that query document 1 and 2 are approximately 4.1% similar. Similarly we compare 1.txt with 2.txt, 3.txt and 4.txt respectively:"""

print("For 3.txt")
file2_docs = []

with open ('3.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file2_docs.append(line)

print("Number of documents:",len(file2_docs))  
for line in file2_docs:
    query_doc = [w.lower() for w in word_tokenize(line)]
    query_doc_bow = dictionary1.doc2bow(query_doc) 
    #update an existing dictionary and create bag of words
# perform a similarity query against the corpus
query_doc_tf_idf = tfidf1[query_doc_bow]
# print(document_number, document_similarity)
print('Comparing Result:', sims1[query_doc_tf_idf]) 
import numpy as np

sum_of_sims =(np.sum(sims1[query_doc_tf_idf], dtype=np.float32))
print(sum_of_sims)
percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))
print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')
print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')
print(f'Average similarity rounded percentage: {percentage_of_similarity}')
print("================================================")
print("================================================")
print("================================================")


print("For 4.txt")
file2_docs = []

with open ('4.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file2_docs.append(line)
# with open ('3.txt') as f:
#     tokens = sent_tokenize(f.read())
#     for line in tokens:
#         file2_docs.append(line)
# with open ('3.txt') as f:
#     tokens = sent_tokenize(f.read())
#     for line in tokens:
#         file2_docs.append(line)

print("Number of documents:",len(file2_docs))  
for line in file2_docs:
    query_doc = [w.lower() for w in word_tokenize(line)]
    query_doc_bow = dictionary1.doc2bow(query_doc) 
    #update an existing dictionary and create bag of words
# perform a similarity query against the corpus
query_doc_tf_idf = tfidf1[query_doc_bow]
# print(document_number, document_similarity)
print('Comparing Result:', sims1[query_doc_tf_idf]) 
import numpy as np

sum_of_sims =(np.sum(sims1[query_doc_tf_idf], dtype=np.float32))
print(sum_of_sims)
percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))
print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')
print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')
print(f'Average similarity rounded percentage: {percentage_of_similarity}')
print("================================================")
print("================================================")
print("================================================")

"""#Here we conclude that documents 1.txt and 3.txt are 1%, and 1.txt and 4.txt are 4% similar. Please refer to the similarity float value mentioned above. Here. we are using 1.txt as query document and the other documents for comparison

# ==============================================================================================================================================

#Now we clubbed 2.txt, 3.txt and 4.txt together and compared it with 1.txt to find average similarity index
"""

avg_sims = [] # array of averages
with open ('4.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file2_docs.append(line)
with open ('2.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file2_docs.append(line)
with open ('3.txt') as f:
    tokens = sent_tokenize(f.read())
    for line in tokens:
        file2_docs.append(line)

# for line in query documents
for line in file2_docs:
        # tokenize words
        query_doc = [w.lower() for w in word_tokenize(line)]
        # create bag of words
        query_doc_bow = dictionary1.doc2bow(query_doc)
        # find similarity for each document
        query_doc_tf_idf = tf_idf[query_doc_bow]
        # print (document_number, document_similarity)
        print('Comparing Result:', sims1[query_doc_tf_idf]) 
        # calculate sum of similarities for each query doc
        sum_of_sims =(np.sum(sims1[query_doc_tf_idf], dtype=np.float32))
        # calculate average of similarity for each query doc
        avg = sum_of_sims / len(file_docs)
        # print average of similarity for each query doc
        print(f'avg: {sum_of_sims / len(file_docs)}')
        # add average values into array
        avg_sims.append(avg)  
   # calculate total average
total_avg = np.sum(avg_sims, dtype=np.float)/len(file2_docs)
    # round the value and multiply by 100 to format it as percentage
print(total_avg)
percentage_of_similarity = round(float(total_avg) * 100)
print("===")
print("===")
print("===")

print("The perecntage of similarity is therefore in %:", percentage_of_similarity)
    # if percentage is greater than 100
    # that means documents are almost same
# if percentage_of_similarity >= 100:
#     percentage_of_similarity = 100
# print(percentage_of_similarity)

"""#Since the average similarity index percentage of similarity when files "2, 3, 4" combined together and compared to 1.txt is 4%, we can conclude that Ms. Lakshmi Sankaran can be hired by Nat Geo's hiring committee"""